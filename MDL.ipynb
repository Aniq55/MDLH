{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import ModelHawkesExpKernLogLik\n",
    "from tick.hawkes import SimuHawkesExpKernels, SimuHawkesMulti, HawkesExpKern\n",
    "from tick.hawkes import HawkesADM4, HawkesCumulantMatching\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import expon\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import bernoulli\n",
    "import pickle\n",
    "import itertools\n",
    "import tensorflow\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_uniform(end_time, decays, gamma):\n",
    "    p = len(gamma)\n",
    "    adjacency =  gamma * np.random.uniform(low=uni_adj_lb, high=uni_adj_ub,size=(p,p))\n",
    "    #print(np.max(np.abs(np.linalg.eigvals(adjacency))))\n",
    "    #print(np.count_nonzero(adjacency) - p)\n",
    "    baseline = np.random.uniform(low=uni_bas_lb, high=uni_bas_ub,size=p)\n",
    "    true = SimuHawkesExpKernels(\n",
    "        adjacency=adjacency, decays=decays, baseline=baseline,\n",
    "        end_time=end_time*2, verbose=False)\n",
    "    obs = SimuHawkesMulti(true, n_simulations=1)\n",
    "    obs.simulate()\n",
    "    data = obs.timestamps[0].copy()\n",
    "    for i in range(p):\n",
    "        data[i] = data[i][data[i] > end_time] - end_time\n",
    "    #print(len(data[0]))\n",
    "    return data,baseline,adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_uniform_multi(end_time, decays, gamma, n_sim):\n",
    "    p = len(gamma)\n",
    "    adjacency =  gamma * np.random.uniform(low=uni_adj_lb, high=uni_adj_ub,size=(p,p))\n",
    "    #print(np.max(np.abs(np.linalg.eigvals(adjacency))))\n",
    "    #print(np.count_nonzero(adjacency) - p)\n",
    "    baseline = np.random.uniform(low=uni_bas_lb, high=uni_bas_ub,size=p)\n",
    "    true = SimuHawkesExpKernels(\n",
    "        adjacency=adjacency, decays=decays, baseline=baseline,\n",
    "        end_time=end_time*2, verbose=False)\n",
    "    obs = SimuHawkesMulti(true, n_simulations=n_sim)\n",
    "    obs.simulate()\n",
    "    data = obs.timestamps.copy()\n",
    "    for j in range(n_sim):\n",
    "        for i in range(p):\n",
    "            data[j][i] = data[j][i][data[j][i] > end_time] - end_time\n",
    "    #print(len(data[0][0]))\n",
    "    return data,baseline,adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(data,s):\n",
    "    p = len(data)\n",
    "    r = np.zeros(p)\n",
    "    for i in range(p):\n",
    "        r[p-i-1] = s%2\n",
    "        s = s//2\n",
    "    r = r==1\n",
    "    dat = [data[i] for i in range(p) if (r[i])]\n",
    "    ls = HawkesExpKern(decays, penalty='none',gofit = 'least-squares',step = 10, max_iter=10000000, tol = 1e-5)\n",
    "    ls.fit(dat)\n",
    "    #print(ls.adjacency)\n",
    "    #print(ls.baseline)\n",
    "    mle = HawkesExpKern(decays, penalty='none', gofit = 'likelihood', step = 1, max_iter=10000000, tol = 1e-5, solver = 'gd')\n",
    "    mle.fit(events=dat,start = ls.coeffs+1e-5)\n",
    "    #print(mle.adjacency)\n",
    "    return mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gamma(p):\n",
    "    gamma = bernoulli(bern_param).rvs((p,p))\n",
    "    for i in range(p):\n",
    "        gamma[i][i] = 1 \n",
    "    gamma = gamma == 1\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gamma_sparse(p,deg):\n",
    "    gamma = np.diag(np.ones(p))\n",
    "    for i in range(p):\n",
    "        u = np.random.choice(range(0,deg+1))\n",
    "        r = np.zeros(p-1)\n",
    "        r[:u] = 1\n",
    "        np.random.shuffle(r)\n",
    "        o = 0\n",
    "        for j in range(p-1):\n",
    "            if (j==i):\n",
    "                o = 1\n",
    "            else:\n",
    "                gamma[i][j+o] = r[j]\n",
    "    gamma = gamma == 1\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_eval(data):\n",
    "    p = len(data)\n",
    "    adj = [0]\n",
    "    bas = [0]\n",
    "    for s in range(1,2**p):\n",
    "        #print(s)\n",
    "        mle = estimate(data,s)\n",
    "        adj.append(mle.adjacency)\n",
    "        bas.append(mle.baseline)\n",
    "    return adj,bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_eval_sparse(data,deg):\n",
    "    p = len(data)\n",
    "    adj = dict()\n",
    "    bas = dict()\n",
    "    tot = []\n",
    "    for u in range(1,deg+2):\n",
    "        for w in itertools.combinations(range(p),u):\n",
    "            tot.append(w)\n",
    "    for w in tot:\n",
    "        s = 0\n",
    "        for c in w:\n",
    "            s += 2**c\n",
    "        mle = estimate(data,s)\n",
    "        adj[s] = mle.adjacency\n",
    "        bas[s] = mle.baseline\n",
    "    return adj,bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_hat_eval(adj,bas,gamma):\n",
    "    p = len(gamma)\n",
    "    a = np.zeros((p,p))\n",
    "    b = np.zeros(p)\n",
    "    for i in range(p):\n",
    "        s = 0\n",
    "        for x in gamma[i]:\n",
    "            s = s*2 + x\n",
    "        u = 0\n",
    "        for j in range(i):\n",
    "            if (gamma[i][j]):\n",
    "                u+=1\n",
    "        b[i] = bas[s][u]\n",
    "        w = 0\n",
    "        for j in range(p):\n",
    "            if (gamma[i][j] == 1):\n",
    "                a[i][j] = adj[s][u][w]\n",
    "                w += 1\n",
    "    coeffs = np.concatenate([b,a.flatten()]) #p+p^2 entries\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(theta_hat,theta,data,gamma_hat,gamma):\n",
    "    L = ModelHawkesExpKernLogLik(decay=decays,n_threads=0)\n",
    "    L.fit(data);\n",
    "    pr_hat = np.exp(-L.loss(coeffs=theta_hat))\n",
    "    pr = np.exp(-L.loss(coeffs=theta))\n",
    "    return (pr_hat/pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COMP(gamma,prep):\n",
    "    prep_data_list,prep_adj_list,prep_bas_list,prep_gamma_list,prep_theta_list = prep\n",
    "    x = []\n",
    "    N = len(prep_adj_list)\n",
    "    for i in range(N):\n",
    "        theta_hat = theta_hat_eval(adj=prep_adj_list[i],bas = prep_bas_list[i],gamma=gamma)\n",
    "        u = q(theta_hat=theta_hat,theta=prep_theta_list[i],data = prep_data_list[i],gamma_hat=gamma,gamma=prep_gamma_list[i])\n",
    "        x.append(u)\n",
    "    #print(np.mean(x))\n",
    "    #print(np.std(x))\n",
    "    return np.log(np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(p,T):\n",
    "    gamma_true= generate_gamma(p)\n",
    "    data_true,baseline_true,adjacency_true = generate_data_uniform(end_time=T,decays=decays,gamma = gamma_true)\n",
    "    return data_true,gamma_true,adjacency_true,baseline_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_sparse(p,T,deg):\n",
    "    gamma_true= generate_gamma_sparse(p,deg)\n",
    "    data_true,baseline_true,adjacency_true = generate_data_uniform(end_time=T,decays=decays,gamma = gamma_true)\n",
    "    return data_true,gamma_true,adjacency_true,baseline_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_multi(p,n_sim,T):\n",
    "    gamma_true= generate_gamma(p)\n",
    "    data_true,baseline_true,adjacency_true = generate_data_uniform_multi(end_time=T,decays=decays,gamma = gamma_true, n_sim=n_sim)\n",
    "    return data_true,gamma_true,adjacency_true,baseline_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_multi_sparse(p,n_sim,T,deg):\n",
    "    gamma_true= generate_gamma_sparse(p,deg)\n",
    "    data_true,baseline_true,adjacency_true = generate_data_uniform_multi(end_time=T,decays=decays,gamma = gamma_true, n_sim=n_sim)\n",
    "    return data_true,gamma_true,adjacency_true,baseline_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prep(p,N,n_sim,T):\n",
    "    data_list = []\n",
    "    adj_list = []\n",
    "    bas_list = []\n",
    "    gamma_list = []\n",
    "    theta_list = []\n",
    "    c = 0\n",
    "    while(c < N):\n",
    "        print(\"data\")\n",
    "        try:\n",
    "            data,gamma,adjacency,baseline = generate_sample_multi(p,n_sim,T)\n",
    "        except Exception:\n",
    "            print(\"Oops! large spr\")\n",
    "            continue\n",
    "        theta = np.concatenate([baseline,adjacency.flatten()])\n",
    "        print(\"fit\")\n",
    "        for d in data:\n",
    "            try:\n",
    "                adj,bas = est_eval(d)\n",
    "            except Exception:\n",
    "                print(\"Oops! can't fit\")\n",
    "                break\n",
    "            c+=1\n",
    "            print(c)\n",
    "            data_list.append(d)\n",
    "            adj_list.append(adj)\n",
    "            bas_list.append(bas)\n",
    "            gamma_list.append(gamma)\n",
    "            theta_list.append(theta)\n",
    "            if (c == N):\n",
    "                break\n",
    "                \n",
    "    prep = [data_list,adj_list,bas_list,gamma_list,theta_list]\n",
    "                \n",
    "    COMP_list = []\n",
    "    for i in range(p):\n",
    "        COMP_list.append([])\n",
    "        for s in range(2**(p-1)):\n",
    "            r = np.zeros(p)\n",
    "            k = s\n",
    "            for j in range(p):\n",
    "                if (p-j-1==i):\n",
    "                    continue\n",
    "                r[p-j-1] = k%2\n",
    "                k = k//2\n",
    "            r[i] = 1\n",
    "            r = r==1\n",
    "            #print(r*1)\n",
    "            gamma = np.diag(np.ones(p)) == 1\n",
    "            gamma[i] = r\n",
    "            C = COMP(gamma,prep)\n",
    "            COMP_list[i].append(C)\n",
    "            \n",
    "        \n",
    "    with open('COMP_'+str(p)+'_'+str(T)+'.pkl', 'wb') as output:\n",
    "        pickle.dump(COMP_list, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prep_sparse(p,N,n_sim,T,deg):\n",
    "    data_list = []\n",
    "    adj_list = []\n",
    "    bas_list = []\n",
    "    gamma_list = []\n",
    "    theta_list = []\n",
    "    c = 0\n",
    "    while(c < N):\n",
    "        print(\"data\")\n",
    "        try:\n",
    "            data,gamma,adjacency,baseline = generate_sample_multi_sparse(p,n_sim,T,deg)\n",
    "        except Exception:\n",
    "            print(\"Oops! large spr\")\n",
    "            continue\n",
    "        theta = np.concatenate([baseline,adjacency.flatten()])\n",
    "        print(\"fit\")\n",
    "        for d in data:\n",
    "            try:\n",
    "                adj,bas = est_eval_sparse(d,deg)\n",
    "            except Exception:\n",
    "                print(\"Oops! can't fit\")\n",
    "                break\n",
    "            c+=1\n",
    "            print(c)\n",
    "            data_list.append(d)\n",
    "            adj_list.append(adj)\n",
    "            bas_list.append(bas)\n",
    "            gamma_list.append(gamma)\n",
    "            theta_list.append(theta)\n",
    "            if (c == N):\n",
    "                break\n",
    "                \n",
    "    prep = [data_list,adj_list,bas_list,gamma_list,theta_list]\n",
    "    \n",
    "    tot = []\n",
    "    for u in range(0,deg+1):\n",
    "        for w in itertools.combinations(range(p),u):\n",
    "            tot.append(w)\n",
    "                \n",
    "    COMP_list = []\n",
    "    for i in range(p):\n",
    "        COMP_list.append(dict())\n",
    "        for w in tot:\n",
    "            s = 0\n",
    "            for c in w:\n",
    "                s += 2**c\n",
    "            r = np.zeros(p)\n",
    "            k = s\n",
    "            for j in range(p):\n",
    "                if (p-j-1==i):\n",
    "                    continue\n",
    "                r[p-j-1] = k%2\n",
    "                k = k//2\n",
    "            r[i] = 1\n",
    "            r = r==1\n",
    "            #print(r*1)\n",
    "            gamma = np.diag(np.ones(p)) == 1\n",
    "            gamma[i] = r\n",
    "            C = COMP(gamma,prep)\n",
    "            COMP_list[i][s] = C\n",
    "            \n",
    "        \n",
    "    with open('COMP_'+str(p)+'_'+str(T)+'_'+'sparse'+'_'+str(deg)+'.pkl', 'wb') as output:\n",
    "        pickle.dump(COMP_list, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_COMP(p,T):\n",
    "    with open('COMP_'+str(p)+'_'+str(T)+'.pkl', 'rb') as input:\n",
    "        li = pickle.load(input)\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_COMP_sparse(p,T,deg):\n",
    "    with open('COMP_'+str(p)+'_'+str(T)+'_'+'sparse'+'_'+str(deg)+'.pkl', 'rb') as input:\n",
    "        li = pickle.load(input)\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(p,T,size):\n",
    "    data_true_list = []\n",
    "    gamma_true_list = []\n",
    "    adjacency_true_list = []\n",
    "    baseline_true_list = []\n",
    "    c = 0\n",
    "    while(c<size):\n",
    "        #print(\"data\")\n",
    "        try:\n",
    "            data_true,gamma_true,adjacency_true,baseline_true = generate_sample(p,T)\n",
    "        except Exception:\n",
    "            print(\"Oops! large spr\")\n",
    "            continue\n",
    "        #print(\"fit\")\n",
    "        try:\n",
    "            adj_true,bas_true = est_eval(data=data_true)\n",
    "        except Exception:\n",
    "            print(\"Oops! can't fit\")\n",
    "            continue\n",
    "        c+=1\n",
    "        print(c)\n",
    "        data_true_list.append(data_true)\n",
    "        gamma_true_list.append(gamma_true)\n",
    "        adjacency_true_list.append(adjacency_true)\n",
    "        baseline_true_list.append(baseline_true)\n",
    "        \n",
    "    with open('dataset_'+str(p)+'_'+str(T)+'.pkl', 'wb') as output:\n",
    "        pickle.dump([data_true_list,gamma_true_list,adjacency_true_list,baseline_true_list,T], output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_sparse(p,T,size,deg):\n",
    "    data_true_list = []\n",
    "    gamma_true_list = []\n",
    "    adjacency_true_list = []\n",
    "    baseline_true_list = []\n",
    "    c = 0\n",
    "    while(c<size):\n",
    "        #print(\"data\")\n",
    "        try:\n",
    "            data_true,gamma_true,adjacency_true,baseline_true = generate_sample_sparse(p,T,deg)\n",
    "        except Exception:\n",
    "            print(\"Oops! large spr\")\n",
    "            continue\n",
    "        #print(\"fit\")\n",
    "        try:\n",
    "            #adj_true,bas_true = est_eval_sparse(data_true,deg)\n",
    "            a = 1\n",
    "        except Exception:\n",
    "            print(\"Oops! can't fit\")\n",
    "            continue\n",
    "        c+=1\n",
    "        print(c)\n",
    "        data_true_list.append(data_true)\n",
    "        gamma_true_list.append(gamma_true)\n",
    "        adjacency_true_list.append(adjacency_true)\n",
    "        baseline_true_list.append(baseline_true)\n",
    "        \n",
    "    with open('dataset_'+str(p)+'_'+str(T)+'_'+'sparse'+'_'+str(deg)+'.pkl', 'wb') as output:\n",
    "        pickle.dump([data_true_list,gamma_true_list,adjacency_true_list,baseline_true_list,T], output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(p,T):\n",
    "    with open('dataset_'+str(p)+'_'+str(T)+'.pkl', 'rb') as input:\n",
    "        li = pickle.load(input)\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_sparse(p,T,deg):\n",
    "    with open('dataset_'+str(p)+'_'+str(T)+'_'+'sparse'+'_'+str(deg)+'.pkl', 'rb') as input:\n",
    "        li = pickle.load(input)\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(gamma_true,gamma_star):\n",
    "    p = len(gamma_true)\n",
    "    g_true = gamma_true\n",
    "    g_star = gamma_star\n",
    "    return np.count_nonzero((g_true*1) * (g_star*1))/np.count_nonzero(g_star*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(gamma_true,gamma_star):\n",
    "    p = len(gamma_true)\n",
    "    g_true = gamma_true\n",
    "    g_star = gamma_star\n",
    "    return np.count_nonzero((g_true*1) * (g_star*1))/np.count_nonzero(g_true*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(gamma_true,gamma_star):\n",
    "    p = precision(gamma_true,gamma_star)\n",
    "    r = recall(gamma_true,gamma_star)\n",
    "    return 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gamma_true_list,gamma_star_list,metric=F1_score):\n",
    "    size = len(gamma_star_list)\n",
    "    res = []\n",
    "    for i in range(size):\n",
    "        sc = metric(gamma_true_list[i],gamma_star_list[i])\n",
    "        res.append(sc)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_method(dataset,method,hyperparams,metric=F1_score,if_print=False):\n",
    "    data_list,gamma_list,adjacency_list,baseline_list,T = dataset\n",
    "    size = len(data_list)\n",
    "    gamma_hat_list = []\n",
    "    for i in range(size):\n",
    "        gamma_hat = method(data_list[i],T,hyperparams)\n",
    "        gamma_hat_list.append(gamma_hat)\n",
    "        sc = metric(gamma_list[i],gamma_hat)\n",
    "        if (if_print):\n",
    "            print(\"TEST \" + str(i+1) + \" : \" + str(sc))\n",
    "    return gamma_hat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results):\n",
    "    with open('results.pkl', 'wb') as output:\n",
    "        pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_results():\n",
    "    with open('results.pkl', 'rb') as input:\n",
    "        li = pickle.load(input)\n",
    "    return li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDLH(data,T,hyperparams=None):\n",
    "    p = len(data)\n",
    "    adj, bas = est_eval(data)\n",
    "    L = ModelHawkesExpKernLogLik(decay=decays,n_threads=0)\n",
    "    L.fit(data);\n",
    "    COMP_list = load_COMP(p,T)\n",
    "    gamma_hat = np.diag(np.ones(p)) == 1\n",
    "    for i in range(p):\n",
    "        score = 1e100\n",
    "        r_hat = []\n",
    "        for s in range(2**(p-1)):\n",
    "            r = np.zeros(p)\n",
    "            k = s\n",
    "            for j in range(p):\n",
    "                if (p-j-1==i):\n",
    "                    continue\n",
    "                r[p-j-1] = k%2\n",
    "                k = k//2\n",
    "            r[i] = 1\n",
    "            r = r==1\n",
    "            #print(r*1)\n",
    "            gamma = np.diag(np.ones(p)) == 1\n",
    "            gamma[i] = r\n",
    "            theta = theta_hat_eval(adj=adj,bas=bas,gamma=gamma)\n",
    "            NML = L.loss(theta) + COMP_list[i][s]\n",
    "            if (NML<score):\n",
    "                score = NML\n",
    "                r_hat = r\n",
    "        gamma_hat[i] = r_hat\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDLH_sparse(data,T,hyperparams):\n",
    "    deg = hyperparams\n",
    "    p = len(data)\n",
    "    adj, bas = est_eval_sparse(data,deg)\n",
    "    L = ModelHawkesExpKernLogLik(decay=decays,n_threads=0)\n",
    "    L.fit(data);\n",
    "    COMP_list = load_COMP_sparse(p,T,deg)\n",
    "    gamma_hat = np.diag(np.ones(p)) == 1\n",
    "    \n",
    "    tot = []\n",
    "    for u in range(0,deg+1):\n",
    "        for w in itertools.combinations(range(p),u):\n",
    "            tot.append(w)\n",
    "    \n",
    "    for i in range(p):\n",
    "        score = 1e100\n",
    "        r_hat = []\n",
    "        for w in tot:\n",
    "            s = 0\n",
    "            for c in w:\n",
    "                s += 2**c\n",
    "            r = np.zeros(p)\n",
    "            k = s\n",
    "            for j in range(p):\n",
    "                if (p-j-1==i):\n",
    "                    continue\n",
    "                r[p-j-1] = k%2\n",
    "                k = k//2\n",
    "            r[i] = 1\n",
    "            r = r==1\n",
    "            #print(r*1)\n",
    "            gamma = np.diag(np.ones(p)) == 1\n",
    "            gamma[i] = r\n",
    "            theta = theta_hat_eval(adj=adj,bas=bas,gamma=gamma)\n",
    "            NML = L.loss(theta) + COMP_list[i][s]\n",
    "            if (NML<score):\n",
    "                score = NML\n",
    "                r_hat = r\n",
    "        gamma_hat[i] = r_hat\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LS(data,T,hyperparams):\n",
    "    p = len(data)\n",
    "    penalty,C = hyperparams\n",
    "    ls = HawkesExpKern(decays,gofit = 'least-squares', step = 10, max_iter=10000, tol = 1e-10, solver = 'gd', C=C)\n",
    "    ls.fit(data)\n",
    "    gamma_hat = ls.adjacency > TH\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE(data,T,hyperparams):\n",
    "    p = len(data)\n",
    "    penalty,C = hyperparams\n",
    "    ls = HawkesExpKern(decays,gofit = 'least-squares', step = 10, max_iter=10000, tol = 1e-5, solver = 'gd')\n",
    "    ls.fit(data)\n",
    "    est = HawkesExpKern(decays, C = C, penalty=penalty, gofit = 'likelihood', step = 10, max_iter=10000, tol = 1e-5, solver = 'gd')\n",
    "    est.fit(events=data,start = ls.coeffs+1e-5)\n",
    "    gamma_hat = est.adjacency > TH\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_methods(data,T,hyperparams):\n",
    "    p = len(data)\n",
    "    adj, bas = est_eval(data)\n",
    "    L = ModelHawkesExpKernLogLik(decay=decays,n_threads=0)\n",
    "    L.fit(data);\n",
    "    ic = hyperparams\n",
    "    n = 0\n",
    "    for i in range(p):\n",
    "        n+= len(data[i])\n",
    "    gamma_hat = np.diag(np.ones(p)) == 1\n",
    "    for i in range(p):\n",
    "        best = 1e100\n",
    "        r_hat = []\n",
    "        for s in range(2**(p-1)):\n",
    "            r = np.zeros(p)\n",
    "            k = s\n",
    "            for j in range(p):\n",
    "                if (p-j-1==i):\n",
    "                    continue\n",
    "                r[p-j-1] = k%2\n",
    "                k = k//2\n",
    "            r[i] = 1\n",
    "            r = r==1\n",
    "            #print(r*1)\n",
    "            gamma = np.diag(np.ones(p)) == 1\n",
    "            gamma[i] = r\n",
    "            theta = theta_hat_eval(adj=adj,bas=bas,gamma=gamma)\n",
    "            ic_penalty = 0\n",
    "            k = (np.sum(r)+p-1 + p)\n",
    "            if (ic == 'AIC'):\n",
    "                ic_penalty = 2*k\n",
    "            if (ic == 'BIC'):\n",
    "                ic_penalty = k*np.log(n)\n",
    "            if (ic == 'HQ'):\n",
    "                ic_penalty = 2*k*np.log(np.log(n))\n",
    "            #print(str(ic_penalty) +  \" \" + str(2*L.loss(theta)))\n",
    "            score = 2*L.loss(theta) + ic_penalty\n",
    "            if (score<best):\n",
    "                best = score\n",
    "                r_hat = r\n",
    "        gamma_hat[i] = r_hat\n",
    "    if (np.sum(gamma_hat*1 - np.diag(np.ones(p))) !=0):\n",
    "        print(\"asdfa\")\n",
    "    #else:\n",
    "    #    print(\":((\")\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_methods_sparse(data,T,hyperparams):\n",
    "    p = len(data)\n",
    "    L = ModelHawkesExpKernLogLik(decay=decays,n_threads=0)\n",
    "    L.fit(data);\n",
    "    ic,deg = hyperparams\n",
    "    print(\"a\")\n",
    "    adj, bas = est_eval_sparse(data,deg)\n",
    "    print(\"b\")\n",
    "    n = 0\n",
    "    for i in range(p):\n",
    "        n+= len(data[i])\n",
    "        \n",
    "    gamma_hat = np.diag(np.ones(p)) == 1\n",
    "    \n",
    "    tot = []\n",
    "    for u in range(0,deg+1):\n",
    "        for w in itertools.combinations(range(p),u):\n",
    "            tot.append(w)\n",
    "            \n",
    "    for i in range(p):\n",
    "        best = 1e100\n",
    "        r_hat = []\n",
    "        for w in tot:\n",
    "            s = 0\n",
    "            for c in w:\n",
    "                s += 2**c\n",
    "            r = np.zeros(p)\n",
    "            k = s\n",
    "            for j in range(p):\n",
    "                if (p-j-1==i):\n",
    "                    continue\n",
    "                r[p-j-1] = k%2\n",
    "                k = k//2\n",
    "            r[i] = 1\n",
    "            r = r==1\n",
    "            #print(r*1)\n",
    "            gamma = np.diag(np.ones(p)) == 1\n",
    "            gamma[i] = r\n",
    "            theta = theta_hat_eval(adj=adj,bas=bas,gamma=gamma)\n",
    "            ic_penalty = 0\n",
    "            k = (np.sum(r)+p-1 + p)\n",
    "            if (ic == 'AIC'):\n",
    "                ic_penalty = 2*k\n",
    "            if (ic == 'BIC'):\n",
    "                ic_penalty = k*np.log(n)\n",
    "            if (ic == 'HQ'):\n",
    "                ic_penalty = 2*k*np.log(np.log(n))\n",
    "            #print(str(ic_penalty) +  \" \" + str(2*L.loss(theta)))\n",
    "            score = 2*L.loss(theta) + ic_penalty\n",
    "            if (score<best):\n",
    "                best = score\n",
    "                r_hat = r\n",
    "        gamma_hat[i] = r_hat\n",
    "    if (np.sum(gamma_hat*1 - np.diag(np.ones(p))) !=0):\n",
    "        print(\"asdfa\")\n",
    "    #else:\n",
    "    #    print(\":((\")\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADM4(data,T,hyperparams):\n",
    "    C, lasso_nuclear_ratio = hyperparams\n",
    "    learner = HawkesADM4(decay=decays,C=C,lasso_nuclear_ratio=lasso_nuclear_ratio)\n",
    "    learner.fit(data)\n",
    "    #print((learner.adjacency>TH)*1)\n",
    "    gamma_hat = (learner.adjacency>TH)\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPHC(data,T,hyperparams):\n",
    "    C, support, penalty = hyperparams\n",
    "    nphc = HawkesCumulantMatching(integration_support = support, C=C, penalty = penalty, tol=1e-10, cs_ratio=.15)\n",
    "    nphc.fit(data)\n",
    "    gamma_hat = (nphc.adjacency>TH)\n",
    "    return gamma_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_adj_lb = 0.1\n",
    "uni_adj_ub = 0.2\n",
    "uni_bas_lb = 0.5\n",
    "uni_bas_ub = 1.0\n",
    "bern_param = 0.3\n",
    "decays = 1\n",
    "TH = 0.01\n",
    "p = 7\n",
    "T = 700\n",
    "N = 1000\n",
    "n_sim = 10\n",
    "size = 100\n",
    "deg = 1\n",
    "metric = F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = dict()\n",
    "save_results(results)\n",
    "#generate_dataset_sparse(p,T,size,deg)\n",
    "generate_dataset(p,T,size)\n",
    "#dataset = load_dataset_sparse(p,T,deg)\n",
    "dataset = load_dataset(p,T)\n",
    "gamma_true_list = dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_list = ['l1','none','l2','elasticnet']\n",
    "C_list = [500,1000,2000,5000,10000,20000,50000,100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = load_results()\n",
    "bestest = 0\n",
    "for penalty in penalty_list:\n",
    "    acc_best = 0\n",
    "    for C in C_list:\n",
    "        if (penalty == 'none'):\n",
    "            C = None\n",
    "        hyperparams = [penalty,C]\n",
    "        gamma_hat_list = run_method(dataset,MLE,hyperparams,metric,True)\n",
    "        res = evaluate(gamma_true_list,gamma_hat_list,metric)\n",
    "        acc = np.mean(res)\n",
    "        if (acc > acc_best):\n",
    "            print(acc)\n",
    "            acc_best = acc\n",
    "        if (penalty == 'none'):\n",
    "            break\n",
    "    print(penalty + \" \" + str(acc_best))\n",
    "    if (bestest < acc_best):\n",
    "        bestest = acc_best\n",
    "        results['likelihood'+'_'+str(p)+'_'+str(T)] = bestest\n",
    "\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_list = ['l1','l2','elasticnet','none']#,'nuclear']\n",
    "C_list = [1,2,5,10,20,50,100,200,500,1000,2000,5000,10000,20000,50000,100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results()\n",
    "\n",
    "bestest = 0\n",
    "for penalty in penalty_list:\n",
    "    acc_best = 0\n",
    "    for C in C_list:\n",
    "        if (penalty == 'none'):\n",
    "            C = None\n",
    "        hyperparams = [penalty,C]\n",
    "        gamma_hat_list = run_method(dataset,LS,hyperparams,metric,True)\n",
    "        res = evaluate(gamma_true_list,gamma_hat_list,metric)\n",
    "        acc = np.mean(res)\n",
    "        if (acc > acc_best):\n",
    "            print(acc)\n",
    "            acc_best = acc\n",
    "        if (penalty == 'none'):\n",
    "            break\n",
    "    print(penalty + \" \" + str(acc_best))\n",
    "    if (bestest < acc_best):\n",
    "        bestest = acc_best\n",
    "        results['least-squares'+'_'+str(p)+'_'+str(T)] = bestest\n",
    "\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1,2,5,10,20,50,100,200,500,1000,2000,5000,10000,20000,50000,100000]\n",
    "ratio_list = [0,0.1,0.5,0.9,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = load_results()\n",
    "\n",
    "best = 0\n",
    "for ratio in ratio_list:\n",
    "    for C in C_list:\n",
    "        hyperparams = [C,ratio]\n",
    "        gamma_hat_list = run_method(dataset,ADM4,hyperparams,metric,True)\n",
    "        res = evaluate(gamma_true_list,gamma_hat_list,metric)\n",
    "        acc = np.mean(res)\n",
    "        if (acc > best):\n",
    "            print(acc)\n",
    "            best = acc\n",
    "        print(str(C) + ' '  + str(ratio) + \" \" + str(acc))\n",
    "\n",
    "results['ADM4'+'_'+str(p)+'_'+str(T)] = best\n",
    "\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [50,100,200,500,1000,2000,5000,10000,20000,50000,100000]\n",
    "penalty_list = ['l1','elasticnet','none', 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results()\n",
    "best= 0\n",
    "for penalty in penalty_list:\n",
    "    acc_best = 0\n",
    "    for C in C_list:\n",
    "        if (penalty == 'none'):\n",
    "            C = None\n",
    "        hyperparams = [C,5,penalty]\n",
    "        gamma_hat_list = run_method(dataset,NPHC,hyperparams,metric)\n",
    "        res = evaluate(gamma_true_list,gamma_hat_list)\n",
    "        acc = np.mean(res)\n",
    "        if (acc > best):\n",
    "            print(acc)\n",
    "            best = acc\n",
    "            results['NPHC'+'_'+str(p)+'_'+str(T)] = best\n",
    "        print(penalty + \" \" + str(C) + \" \" + str(best))\n",
    "        if (penalty == 'none'):\n",
    "          break\n",
    "        \n",
    "\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_prep(p,N,n_sim,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = load_results()\n",
    "gamma_hat_list = run_method(dataset,MDLH,None,metric, True)\n",
    "res = evaluate(gamma_true_list,gamma_hat_list,metric)\n",
    "acc = np.mean(res)\n",
    "results['MDLH'+'_'+str(p)+'_'+str(T)] = acc\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate(gamma_true_list,gamma_hat_list,metric)\n",
    "acc = np.mean(res)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDLH sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_prep_sparse(p,N,n_sim,T,deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = load_results()\n",
    "gamma_hat_list = run_method(dataset,MDLH_sparse,deg,metric,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate(gamma_true_list,gamma_hat_list,metric)\n",
    "acc = np.mean(res)\n",
    "results['MDLH_sparse'+'_'+str(p)+'_'+str(T)] = acc\n",
    "print(acc)\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IC Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the log-likelihood value for the most restricted model (empty graph, only baseline vector), and the least restricted model (full graph, ML). The difference in short data is so tiny that no information criterion allow for any model other than empty graph. The difference in likelihood is of order 0.01 while it should be at least 1 so that IC allows for increasing the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = ['AIC', deg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ModelHawkesExpKernLogLik(decay=decays,n_threads=0)\n",
    "L.fit(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_ML=MLE(data,T,['none',None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = estimate(data,2**p-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = []\n",
    "for i in range(p):\n",
    "    v.append(len(data[i])/T)\n",
    "for i in range(p*p):\n",
    "    v.append(0)\n",
    "v = np.array(v)\n",
    "L.loss(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.loss(ml.coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.loss(v) - L.loss(ml.coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real-data Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"ddly-data.csv\",delimiter=\";\")\n",
    "raw = raw.drop(['dd/mm/yy'],axis = 1)\n",
    "bonds = [ 'CAN_b', 'US_b', 'UK_b', 'GER_b', 'FRA_b', 'ITA_b', 'JPN_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.filter(bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_shock(a,i,k):\n",
    "    n = len(a)\n",
    "    if (i<k):\n",
    "        k = i\n",
    "    thresh = np.sort(a[(i-k):i+1])[::-1][int(k/5)]\n",
    "    return a[i]>thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 250\n",
    "def get_events(data):\n",
    "    n = len(data)\n",
    "    nodes = list(data.columns)\n",
    "    events = []\n",
    "    for x in nodes:\n",
    "        print(x)\n",
    "        event = [False for i in range(n)]\n",
    "        for i in range(n):\n",
    "            event[i] = is_shock(data[x],i,window_size)\n",
    "        tick = np.where(event)[0].tolist()\n",
    "        events.append(np.asarray(tick).astype(np.double))\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(raw.columns)\n",
    "p = len(nodes)\n",
    "T = len(raw)\n",
    "data = get_events(raw).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(p):\n",
    "    print(len(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(p):\n",
    "    print(len(dataset[0][0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(p):\n",
    "    data[i] = 400 * data[i]/T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj,bas = est_eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_star = MDLH(data,400,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_star*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 std, 120 w\n",
    "for i in range(p):\n",
    "    for j in range(p):\n",
    "        if (gamma_star[i][j] and i!=j):\n",
    "            print(bonds[j] + \" to \" + bonds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_mle = MLE(data,400,['l1',80])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(p):\n",
    "    for j in range(p):\n",
    "        if (gamma_mle[i][j] and i!=j):\n",
    "            print(bonds[j] + \" to \" + bonds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
